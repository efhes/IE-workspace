{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JQ-yIj9fH0N"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previous block must be executed in advance so as to ensure that the visualization of the different figures work well."
      ],
      "metadata": {
        "id": "hB1pA0sUOrmO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWZUA8tSfH0R"
      },
      "source": [
        "\n",
        "# Transfer Learning for Computer Vision Tutorial\n",
        "**Author**: Fernando Fernández-Martínez (based on a previous work by [Sasank Chilamkurthy](https://chsasank.github.io))\n",
        "\n",
        "In this tutorial, we will learn how to train a convolutional neural network (ConvNet or CNN) for image classification using transfer learning (TL). You can read more about the transfer\n",
        "learning at [cs231n notes](https://cs231n.github.io/transfer-learning/).\n",
        "\n",
        "Quoting these notes,\n",
        "\n",
        "    In practice, very few people train an entire Convolutional Network\n",
        "    from scratch (with random initialization), because it is relatively\n",
        "    rare to have a dataset of sufficient size. Instead, it is common to\n",
        "    pretrain a ConvNet on a very large dataset (e.g. ImageNet, which\n",
        "    contains 1.2 million images with 1000 categories), and then use the\n",
        "    ConvNet either as an initialization or a fixed feature extractor for\n",
        "    the task of interest.\n",
        "\n",
        "These two major transfer learning scenarios look as follows:\n",
        "\n",
        "-  **Finetuning the convnet**: Instead of random initialization, we\n",
        "   initialize the network with a pretrained network, like the one that is\n",
        "   trained on [Imagenet](https://www.image-net.org/update-mar-11-2021.php) 1000 dataset. Rest of the training looks as\n",
        "   usual.\n",
        "-  **ConvNet as fixed feature extractor**: Here, we will freeze the weights\n",
        "   for all of the network except that of the final fully connected\n",
        "   layer. This last fully connected layer is replaced with a new one\n",
        "   with random weights and only this layer is trained.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0) Computer vision libraries in PyTorch"
      ],
      "metadata": {
        "id": "RCHF1ovMgYOj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11_rqNGTfH0T"
      },
      "outputs": [],
      "source": [
        "# License: BSD\n",
        "# Author: Fernando Fernandez-Martinez (based on previous work by Sasank Chilamkurthy)\n",
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "\n",
        "# torchvision\tlibrary: it contains datasets, model architectures and image transformations often used for computer vision problems.\n",
        "import torchvision\n",
        "\n",
        "# torchvision.datasets: here you'll find many example computer vision datasets for a range of problems from image classification,\n",
        "# object detection, image captioning, video classification and more. It also contains a series of base classes for making custom datasets.\n",
        "\n",
        "# torchvision.models:\tthis module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, \n",
        "# you can use these with your own problems.\n",
        "\n",
        "# torchvision.transforms:\toften images need to be transformed (turned into numbers/processed/augmented) before being used with a model, \n",
        "# common image transformations are found here.\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "# upload external file before import\n",
        "from google.colab import files\n",
        "\n",
        "cudnn.benchmark = True\n",
        "plt.ion()   # interactive mode"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1) Connect to your Google Drive and unzip required data\n",
        "This notebook will initially request access to your Google Drive files. You should give it access to Google Drive in order to mount it and access its content. By giving such access, the code running in the notebook will be able to modify the files in your Google Drive (this is mandatory to be able to download the models that will result from the training process)."
      ],
      "metadata": {
        "id": "EM1FDWBS9qxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "x178UorVfphc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First run of this notebook will require you to unzip the dataset that we are going to use as a baseline to become familiar with the training process.\n",
        "\n",
        "Subsequent runs of the notebook can simply overlook this step by commenting the line. It is possible to comment out a line of code by simply adding a preceding #.\n",
        "\n",
        "NOTE ::\n",
        "   You can directly download the data from\n",
        "   [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip).\n",
        "   Then you just have to move the .zip file to a folder named:\n",
        "\n",
        "> **INSE_SmartRecycling/data/**\n",
        "   \n",
        "   The notebook will automatically extract its content to the current directory.\n"
      ],
      "metadata": {
        "id": "QWp-4dxQ8o2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/gdrive/MyDrive/INSE_SmartRecycling/data/hymenoptera_data.zip -d data/"
      ],
      "metadata": {
        "id": "M9k5uWWOgmnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTfFF6sffH0U"
      },
      "source": [
        "# Step 2) Getting data ready\n",
        "\n",
        "We will use torchvision and torch.utils.data packages for loading the\n",
        "data.\n",
        "\n",
        "First, we will start by solving the problem of training a model to classify\n",
        "**ants** and **bees**. We have about 120 training images each for ants and bees.\n",
        "There are 75 validation images for each class. Usually, this is a very\n",
        "small dataset to generalize upon, if trained from scratch. Since we\n",
        "are using transfer learning, we should be able to generalize reasonably\n",
        "well.\n",
        "\n",
        "This dataset is a very small subset of imagenet.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "uU_CIRyRg4iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1)  Getting the dataset\n",
        "DO NOT FORGET TO CHANGE `notebook_run` ACCORDINGLY!!!\n",
        "\n",
        "Please, leave **just one** of the next alternative lines uncommented."
      ],
      "metadata": {
        "id": "1pgNeuSWhxbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_run = 1\n",
        "if notebook_run == 0:\n",
        "  # Baseline example to familiarize with all the stuff\n",
        "  data_dir = 'data/hymenoptera_data'\n",
        "else:\n",
        "  # After completing a first run we will check the effect of using our own data\n",
        "  data_dir = '/content/gdrive/MyDrive/INSE_SmartRecycling/EXERCISE/'"
      ],
      "metadata": {
        "id": "yVT6VNP1g9Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}"
      ],
      "metadata": {
        "id": "9aeMXkOXi-3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2) Prepare DataLoader\n",
        "\n",
        "Now we've got a dataset ready to go.\n",
        "The next step is to prepare it with a torch.utils.data.DataLoader or DataLoader for short. The DataLoader does what you think it might do: **it helps load data into a model**, for training and for inference.\n",
        "\n",
        "It turns a large Dataset into a Python iterable of smaller chunks.\n",
        "These smaller chunks are called **batches** or **mini-batches** and can be set by the **batch_size** parameter.\n",
        "\n",
        "Why do this? Because it's more computationally efficient. \n",
        "In an ideal world you could do the forward pass and backward pass across all of your data at once.\n",
        "But once you start using really large datasets, unless you've got infinite computing power, it's easier to break them up into batches. \n",
        "It also gives your model more opportunities to improve.\n",
        "With mini-batches (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).\n",
        "\n",
        "What's a good batch size?\n",
        "\n",
        "32 is a good place to start for a fair amount of problems.\n",
        "But since this is a value you can set (a hyperparameter) you can try all different kinds of values, though generally powers of 2 are used most often (e.g. 32, 64, 128, 256, 512)."
      ],
      "metadata": {
        "id": "yQvA-tydi_Zf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_kNlwz5fH0U"
      },
      "outputs": [],
      "source": [
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                              batch_size=4,\n",
        "                                              shuffle=True,\n",
        "                                              num_workers=4) for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3) Define the problem classes\n",
        "DO NOT FORGET TO CHANGE `class_names` ACCORDINGLY!!!\n"
      ],
      "metadata": {
        "id": "MLfPNQ99iDF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if notebook_run == 0:\n",
        "  class_names = image_datasets['train'].classes\n",
        "else:\n",
        "  class_names = ['brontosaurus', 'elephant', 'rhino', 'stegosaurus']\n",
        "  #class_names = ['bottle', 'mouse', 'pencilcase', 'raspberry']"
      ],
      "metadata": {
        "id": "_CxiYcDPhlTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLof5-5mfH0V"
      },
      "source": [
        "### 2.4) Visualize a few images\n",
        "Let's visualize a few training images so as to understand the data\n",
        "augmentations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL6CwTWdfH0V"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3) Functionizing training and test loops"
      ],
      "metadata": {
        "id": "z7dccxRQw2zP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVTD9GtafH0V"
      },
      "source": [
        "## 3.1) Training the model\n",
        "\n",
        "Now, let's write a general function to train a model. Here, we will\n",
        "illustrate:\n",
        "\n",
        "-  Scheduling the learning rate\n",
        "-  Saving the best model\n",
        "\n",
        "In the following, parameter ``scheduler`` is an LR scheduler object from\n",
        "``torch.optim.lr_scheduler``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8lsQ_axfH0W"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    train_losses, test_losses = [], []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train': \n",
        "              train_losses.append(epoch_loss)\n",
        "            else:\n",
        "              test_losses.append(epoch_loss)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_losses, test_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjWdu8NgfH0W"
      },
      "source": [
        "## 3.2) Visualizing the model predictions\n",
        "\n",
        "Generic function to display predictions for a few images\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llz8fOK_fH0X"
      },
      "outputs": [],
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
        "                imshow(inputs.cpu().data[j])\n",
        "                print(f'predicted: {class_names[preds[j]]} ({preds[j]})')\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sghKb9QVfH0X"
      },
      "source": [
        "# Step 4) Finetuning the convnet\n",
        "\n",
        "Load a pretrained model and reset final fully connected layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if notebook_run == 0:\n",
        "  model_ft = models.resnet18(pretrained=True)\n",
        "else:\n",
        "  model_ft = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "print(model_ft)"
      ],
      "metadata": {
        "id": "mFC7u7LVmHMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD1qvQ7DfH0X"
      },
      "outputs": [],
      "source": [
        "if notebook_run == 0:\n",
        "  # First run: baseline Resnet model\n",
        "  # We are about to modify the last layer from:\n",
        "  # (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
        "  # to:\n",
        "  # (fc): Linear(in_features=512, out_features=2, bias=True)\n",
        "  # since our classification problem exactly involves 2 different classes (ants & bees)\n",
        "  num_ftrs = model_ft.fc.in_features\n",
        "  model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "else:\n",
        "  # Second run of the notebook: we adapt the classification layer to match MobileNet\n",
        "  # We are about to modify the last layer from:\n",
        "  # (classifier): Sequential(\n",
        "  #  (0): Dropout(p=0.2, inplace=False)\n",
        "  # -> (1): Linear(in_features=1280, out_features=1000, bias=True) <-\n",
        "  # to:\n",
        "  # (1): Linear(in_features=1280, out_features=4, bias=True)\n",
        "  # since our new custom classification problem exactly involves 4 different classes (the ones you've chosen)\n",
        "  num_ftrs = model_ft.classifier[1].in_features\n",
        "  model_ft.classifier[1] = nn.Linear(num_ftrs, len(class_names)) \n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZaQ7-2JfH0Y"
      },
      "source": [
        "### 4.1) Train and evaluate\n",
        "\n",
        "It should take around 15-25 min on CPU. On GPU though, it takes less than a\n",
        "minute.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCE3znqofH0Y"
      },
      "outputs": [],
      "source": [
        "model_ft, train_losses, val_losses = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2) Make and evaluate random predictions with best model"
      ],
      "metadata": {
        "id": "uhO_E0dXwEzC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozypNBlifH0Y"
      },
      "outputs": [],
      "source": [
        "visualize_model(model_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3) Visualizing training and validation losses"
      ],
      "metadata": {
        "id": "m2N2BKaEvp8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(val_losses, label='Validation loss')\n",
        "plt.legend(frameon=False)"
      ],
      "metadata": {
        "id": "vJY5z11jIf6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4.4) Saving the model"
      ],
      "metadata": {
        "id": "L1lrC-G4vQCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if notebook_run == 0:\n",
        "  model_filename = 'model_ft.pth'\n",
        "else:\n",
        "  model_filename = 'custom_model_ft.pth'\n",
        "\n",
        "torch.save(model_ft.state_dict(), model_filename)\n",
        "\n",
        "# download checkpoint file\n",
        "files.download(model_filename)"
      ],
      "metadata": {
        "id": "w3hBehCEuXpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTO4dEZLfH0Z"
      },
      "source": [
        "# Step 5) ConvNet as fixed feature extractor\n",
        "\n",
        "Here, we need to freeze all the network except the final layer. We need\n",
        "to set ``requires_grad = False`` to freeze the parameters so that the\n",
        "gradients are not computed in ``backward()``.\n",
        "\n",
        "You can read more about this in the documentation\n",
        "[here](https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw3PN8oCfH0Z"
      },
      "outputs": [],
      "source": [
        "if notebook_run == 0:\n",
        "  model_conv = models.resnet18(pretrained=True)\n",
        "else:\n",
        "  model_conv = torchvision.models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "print(model_conv)\n",
        "\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "\n",
        "if notebook_run == 0:\n",
        "  # First run: baseline Resnet model\n",
        "  num_ftrs = model_conv.fc.in_features\n",
        "  model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "else:\n",
        "  # Second run of the notebook: we adapt the classification layer to match MobileNet\n",
        "  num_ftrs = model_conv.classifier[1].in_features\n",
        "  model_conv.classifier[1] = nn.Linear(num_ftrs, len(class_names)) \n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as opposed to before.\n",
        "if notebook_run == 0:\n",
        "  optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "else:\n",
        "  optimizer_conv = optim.SGD(model_conv.classifier[1].parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm4sPxiifH0Z"
      },
      "source": [
        "### 5.1) Train and evaluate\n",
        "\n",
        "On CPU this will take about half the time compared to previous scenario.\n",
        "This is expected as gradients don't need to be computed for most of the\n",
        "network. However, forward does need to be computed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZoQxChyfH0a"
      },
      "outputs": [],
      "source": [
        "model_conv, train_losses, val_losses = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2) Make and evaluate random predictions with best model"
      ],
      "metadata": {
        "id": "e79HavtpwSkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSM9LE8lfH0a"
      },
      "outputs": [],
      "source": [
        "visualize_model(model_conv)\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3) Visualizing training and validation losses"
      ],
      "metadata": {
        "id": "2zMS98SrvjnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(val_losses, label='Validation loss')\n",
        "plt.legend(frameon=False)"
      ],
      "metadata": {
        "id": "Rk-sh-FWJ__v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4) Saving the model"
      ],
      "metadata": {
        "id": "_LXs8dlTvbyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if notebook_run == 0:\n",
        "  model_filename = 'model_conv.pth'\n",
        "else:\n",
        "  model_filename = 'custom_model_conv.pth'\n",
        "\n",
        "torch.save(model_conv.state_dict(), model_filename)\n",
        "\n",
        "# download checkpoint file\n",
        "files.download(model_filename)"
      ],
      "metadata": {
        "id": "Htbza5J7vWtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6) Training a custom model\n",
        "\n",
        "Go back to Step 2), arrange your own custom dataset and re-run the notebook with `notebook_run = 1` to train your Raspberry Pi models. "
      ],
      "metadata": {
        "id": "nlS2dLrPQBQr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BauoNpSfH0a"
      },
      "source": [
        "# Step 7) Further Learning\n",
        "\n",
        "If you would like to learn more about the applications of transfer learning,\n",
        "checkout our [Quantized Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html).\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}